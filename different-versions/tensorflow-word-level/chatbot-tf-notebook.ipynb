{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chatbot-tf-notebook.ipynb","provenance":[],"collapsed_sections":["RAGxzRw0bopk"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8LXbeY52AhPb","colab_type":"text"},"source":["# A Seq2seq Chatbot utilizing an Attention-based Encoder-Decoder Architecture\n"]},{"cell_type":"markdown","metadata":{"id":"HhDJj4tnAszi","colab_type":"text"},"source":["## Environment"]},{"cell_type":"markdown","metadata":{"id":"JhOS8B8J1El0","colab_type":"text"},"source":["We uploaded all the local data in our Google Drive. \n","\n","We will use Google Colaboratory since a GPU accelerator is provided without cost, along with a 26GB RAM too, which both will help us with our computations.\n","\n","Let's mount our data to Google Colaboratory so the notebook can have access to them."]},{"cell_type":"code","metadata":{"id":"9-EzZMQux2Ho","colab_type":"code","outputId":"172c228d-051b-4adc-a9a4-60a3b3369a14","executionInfo":{"status":"ok","timestamp":1569844164565,"user_tz":-180,"elapsed":905511,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_yF3cRxOyt80","colab_type":"code","outputId":"eb96f1da-b057-46f4-8b62-a8f4d8285db0","executionInfo":{"status":"ok","timestamp":1569844166475,"user_tz":-180,"elapsed":907389,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!ls \"/content/drive/My Drive/Colab Notebooks/ncsr/\"\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["chatbot-tf-notebook.ipynb  old\t\t\t version-keras-char-level\n","data\t\t\t   training_checkpoints  version-keras-word-level\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hH3FFN1tyLvL","colab_type":"code","outputId":"a7710409-1309-4dc2-c6ff-15f5087301ab","executionInfo":{"status":"ok","timestamp":1569844169670,"user_tz":-180,"elapsed":910558,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["\n","import os\n","!pwd\n","path_to_mount = '/content/drive/My Drive/Colab Notebooks/ncsr/'\n","os.chdir(path_to_mount)\n","!ls\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n","chatbot-tf-notebook.ipynb  old\t\t\t version-keras-char-level\n","data\t\t\t   training_checkpoints  version-keras-word-level\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VDmvOhAiBIYx","colab_type":"text"},"source":["## JSON Parsing"]},{"cell_type":"markdown","metadata":{"id":"iuDYmns_1agW","colab_type":"text"},"source":["Great, we can now see our notebook and the data folder.\n","\n","By exploring our data, we can see that we are interested in the text files inside the *dialogue folder* and more specifically in the '*turns*' value.\n","\n","If we visualize the files, we can understand that they are JSON formatted.\n","\n","However, they are ill-formatted, meaning that the files themselves are not JSONs.\n","\n","Instead, **each line is a json object itself.**\n","\n","Let's have a look at all the files and parse them with the *json* and *glob* library"]},{"cell_type":"code","metadata":{"id":"APDYTsTC8RQd","colab_type":"code","outputId":"a5827b2c-8bdc-460c-d43f-7a8eb24601fa","executionInfo":{"status":"ok","timestamp":1569844171941,"user_tz":-180,"elapsed":912802,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Import libraries\n","\n","# Parsing \n","import glob\n","import json\n","import random \n","import numpy as np\n","import pandas as pd \n","\n","# Preprocessing & NNs\n","from keras.models import Sequential, Model, load_model\n","from keras.layers import LSTM,Dense, Dropout, Embedding, CuDNNLSTM, Bidirectional, Embedding, Input, TimeDistributed\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","import re\n","import tensorflow as tf\n","tf.enable_eager_execution() # evaluates operations immediately without building graphs\n","# The above does not work with placeholders\n","\n","# Etc\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import seaborn as sns\n","import os\n","import time\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"O5giGPAL3v3a","colab_type":"code","outputId":"429bf8ca-b4d1-46f7-d105-adac4015e5d5","executionInfo":{"status":"ok","timestamp":1569844172763,"user_tz":-180,"elapsed":792,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# Get absolute paths of files\n","dialogues_regex_folder_path = \"data/dialogues/*.txt\"\n","\n","# Get the absolute paths for each file \n","list_of_files = glob.glob(path_to_mount + dialogues_regex_folder_path)\n","print(list_of_files[:3]) # Visualize the first 3\n","print(len(list_of_files)) # 47; crashing? Try lower numbers\n","\n","#list_of_files = list_of_files[5] \n","#list_of_files = random.choices(list, k=5)\n","#print(secrets.randbelow(len(list_of_files)))\n","#list_of_files = random.choices(population=list_of_files, k=5)\n","\n","print(len(list_of_files))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['/content/drive/My Drive/Colab Notebooks/ncsr/data/dialogues/AGREEMENT_BOT.txt', '/content/drive/My Drive/Colab Notebooks/ncsr/data/dialogues/APARTMENT_FINDER.txt', '/content/drive/My Drive/Colab Notebooks/ncsr/data/dialogues/CHECK_STATUS.txt']\n","47\n","47\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TL7AmST0yfIj","colab_type":"code","colab":{}},"source":["# Parsing\n","list_of_dicts = [] # Init\n","\n","# Loop for each file\n","for filename in list_of_files:\n","  with open(filename) as f:\n","      for line in f: # Loop for each line (inside each file)\n","          list_of_dicts.append(json.loads(line)) # insert in a dictionary\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERTrgueE4vmr","colab_type":"code","outputId":"4bfec85e-1c4b-4a02-c212-ef0be5b98446","executionInfo":{"status":"ok","timestamp":1569844186255,"user_tz":-180,"elapsed":13916,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Visualization\n","print(list_of_dicts[0])\n","print(list_of_dicts[1].keys)\n","print(list_of_dicts[332])\n","print(list_of_dicts[:3])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'id': 'c399a493', 'user_id': 'c05f0462', 'bot_id': 'c96edf42', 'domain': 'AGREEMENT_BOT', 'task_id': 'a9203a2c', 'turns': ['Hello how may I help you?', 'i am awesome', 'of course you are', 'and i own rental properties on the moon', 'i doubt you own a property in the moon', 'just kidding. i own them on Earth', \"that's a nice joke\", 'because i am a billionaire!', \"i don't seem to know you\", 'and i programmed you', 'i am the programmer']}\n","<built-in method keys of dict object at 0x7f55d12863a8>\n","{'id': '77d8f493', 'user_id': '3205aff7', 'bot_id': 'f3420389', 'domain': 'AGREEMENT_BOT', 'task_id': 'd47b54df', 'turns': ['Hello how may I help you?', 'I must say that an agreement bot is really useless', 'I have never heard a more correct statement in my entire life!', 'yes what useless trip agreement bots are *tripe', \"I couldn't have said it better myself. Is there anything else I can enthusiastically agree with today?\", 'Well I can repeat my sentiment 4 times about agreement bots being utterly useless', 'That sounds like a fantastic idea!', 'I see you agree wholeheartedly!', 'Yes, absolutely. Every circuit in my mechanical brain agrees with every single thing you say.', 'ok glad you agree', 'Likewise']}\n","[{'id': 'c399a493', 'user_id': 'c05f0462', 'bot_id': 'c96edf42', 'domain': 'AGREEMENT_BOT', 'task_id': 'a9203a2c', 'turns': ['Hello how may I help you?', 'i am awesome', 'of course you are', 'and i own rental properties on the moon', 'i doubt you own a property in the moon', 'just kidding. i own them on Earth', \"that's a nice joke\", 'because i am a billionaire!', \"i don't seem to know you\", 'and i programmed you', 'i am the programmer']}, {'id': '2888aa3e', 'user_id': '46fe62d7', 'bot_id': 'bcc50983', 'domain': 'AGREEMENT_BOT', 'task_id': 'd47b54df', 'turns': ['Hello how may I help you?', 'I am the king of the world', 'I agree that you are the king of the world', 'I can have any woman I want!', 'I agree that you can have any woman you desire.', 'Even you bot, if I were in to AIs', 'Agreed.', \"Really? you're awfully agreeable aren't you\", 'I agree that I am awfully agreeable, yes.', 'Having an agreement bot seems like a useless thing to have. I need some spice in my life!', 'I really agree with that. I am rather useles.']}, {'id': '17a8685a', 'user_id': 'f840ce6a', 'bot_id': '97fcd3ba', 'domain': 'AGREEMENT_BOT', 'task_id': '83ad6a66', 'turns': ['Hello how may I help you?', 'Do you that I am a great person?', 'Yes!', 'I am only 6 inches tall.', \"That's correct!\", 'When I speak the whole world stops to listen to what I say', 'You can count on it.', 'I am the Dalai Lama and I am also the Pope', 'What an accomplishment!', 'I have more money than Bill Gares *Gates', 'Yes you do.']}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1KeWnxC5A3rG","colab_type":"text"},"source":["Great! We have a dictionary out of our raw text dataset.\n","\n","As we can see, we are only interested in the 'turns' value since this contains the array with the QAs.\n","\n","So, we will dump out all the extra properties and create a new dictionary containing only the useful data."]},{"cell_type":"code","metadata":{"id":"deFpLw8p8QqX","colab_type":"code","colab":{}},"source":["# Create a new dict containing only useful data\n","new_list_of_dicts = [] \n","\n","for old_dict in list_of_dicts:\n","  foodict = {k: v for k, v in old_dict.items() if (k == 'turns')} \n","  new_list_of_dicts.append(foodict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCbLc2Oi-wJe","colab_type":"code","outputId":"19a0bf00-b86d-460d-dcfd-b8cae700570c","executionInfo":{"status":"ok","timestamp":1569844187125,"user_tz":-180,"elapsed":845,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["print(new_list_of_dicts[0])\n","print(new_list_of_dicts[332])\n","print(len(new_list_of_dicts))\n","\n","# Just to be sure we don't make bad use of the old variable,\n","# we will make the old dict equal to the new one.\n","# In the end, they are all the same.\n","list_of_dicts = []\n","list_of_dicts = new_list_of_dicts \n","\n","print(list_of_dicts[:2])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'turns': ['Hello how may I help you?', 'i am awesome', 'of course you are', 'and i own rental properties on the moon', 'i doubt you own a property in the moon', 'just kidding. i own them on Earth', \"that's a nice joke\", 'because i am a billionaire!', \"i don't seem to know you\", 'and i programmed you', 'i am the programmer']}\n","{'turns': ['Hello how may I help you?', 'I must say that an agreement bot is really useless', 'I have never heard a more correct statement in my entire life!', 'yes what useless trip agreement bots are *tripe', \"I couldn't have said it better myself. Is there anything else I can enthusiastically agree with today?\", 'Well I can repeat my sentiment 4 times about agreement bots being utterly useless', 'That sounds like a fantastic idea!', 'I see you agree wholeheartedly!', 'Yes, absolutely. Every circuit in my mechanical brain agrees with every single thing you say.', 'ok glad you agree', 'Likewise']}\n","37884\n","[{'turns': ['Hello how may I help you?', 'i am awesome', 'of course you are', 'and i own rental properties on the moon', 'i doubt you own a property in the moon', 'just kidding. i own them on Earth', \"that's a nice joke\", 'because i am a billionaire!', \"i don't seem to know you\", 'and i programmed you', 'i am the programmer']}, {'turns': ['Hello how may I help you?', 'I am the king of the world', 'I agree that you are the king of the world', 'I can have any woman I want!', 'I agree that you can have any woman you desire.', 'Even you bot, if I were in to AIs', 'Agreed.', \"Really? you're awfully agreeable aren't you\", 'I agree that I am awfully agreeable, yes.', 'Having an agreement bot seems like a useless thing to have. I need some spice in my life!', 'I really agree with that. I am rather useles.']}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q_OZcXl0BknR","colab_type":"text"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"U8AYyQsT-_U9","colab_type":"text"},"source":["As we see, we now have a new list of dicts which is a list of 37884 dictionaries (if all 47 texts are loaded) that inside contain only one property ('turns'), and each property contains an array with the QA dataset.\n","\n","Notice that the dialog is instantiated by the bot first, and then the human responds. This goes on and on like ping pong and essentially the dialog is over.\n","\n","**If we observe the data more carefully, we can see that the last sentence may be given by both the bot and the user!**\n","\n","This will come in handy when preparing our input, target dataset.\n","\n","Let's assign the dialogs into 2 matrices:\n","- questions \n","- answers\n","\n","But first, some corner cases need to be defined:\n","- A first 'artificial' user 'greeting' to the bot\n","- An 'artificial' bot 'bye' to the user, if the user was the last one in the dialogue."]},{"cell_type":"code","metadata":{"id":"ZA1t_yp_4s6L","colab_type":"code","colab":{}},"source":["# Init matrices\n","questions = []\n","answers = []\n","\n","# We assume that the first answer by the bot (aka \"Hello, how may I help you?\") \n","# is returned after a user greeting.\n","# This is used in order to ensure that the dataset will be even \n","# and each question is paired with an answer.\n","# That's why we create a mini random catalog \n","# of artificial 'ghost' user greetings.\n","#matrix_greetings = [\"Hey\", \"Hi\", \" \"]\n","matrix_greetings = [\"Hey\", \"Hi\"]\n","\n","# A similar situation happens in the corner case \n","# when the last sentence is from the user.\n","# As said, each sentence from the user should be paired\n","# with a sentence from the bot.\n","# That's why we will in this case add an artificial one.\n","#matrix_byes = [\"Ok\", \"Okie\", \" \", \" \", \"Bye\"]\n","matrix_byes = [\"Ok\", \"Okie\", \"Bye\"]\n","\n","# For each dictionary in the list\n","for dictionary in list_of_dicts:\n","  matrix_QA = dictionary['turns']\n","  \n","  # Append a first random greeting, as explained above\n","  questions.append(random.choice(matrix_greetings))\n","    \n","  # In order to split the QAs to 2 matrices (questions & answers),\n","  # we will use a flag to indicate if the sentence \n","  # is given from the bot or from the user\n","  bot_flag = True # Init\n","\n","  # For each Q/A in the matrix\n","  for sentence in matrix_QA:\n","\n","    if bot_flag == True:\n","      answers.append(sentence) # Used for bot's answers\n","      bot_flag = False # Switch\n","      continue\n","    else:\n","      questions.append(sentence) # Used for user's questions\n","      bot_flag = True # Switch\n","      continue\n","\n","  # The last loop (ideally) ends with a bot's answer,\n","  # thus making bot_flag equal to False.\n","  # Although, with data visualization and exploring,\n","  # we can see that this does not happen all the time.\n","\n","  # Corner case: If the last answers was from the user, \n","  # then we need to add one artificial 'ghost' response \n","  # from the bot to make the dataset even.\n","  if bot_flag == True: \n","    answers.append(random.choice(matrix_byes))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMbXIgqdAWKn","colab_type":"code","outputId":"9a3dbc49-5e3d-41c0-deb6-288600a1a8fd","executionInfo":{"status":"ok","timestamp":1569844187387,"user_tz":-180,"elapsed":1065,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["assert len(questions) == len(answers), \"ERROR: The length of the questions and answer matrices are different.\"\n","# If it does not return any warning/error, then everything is good.\n","\n","print(len(questions)) # We have 238051 QAs (if we load all 47 texts)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["238051\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v-745_oMIkdL","colab_type":"code","outputId":"8794a31a-c186-4443-852b-e209361157f0","executionInfo":{"status":"ok","timestamp":1569844188505,"user_tz":-180,"elapsed":636,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Due to really high memory usage on TensorFlow training,\n","# we need to keep a lower number of dialogs.\n","# Also, we will shuffle them to ensure that our bot isn't overfitting on\n","# limited goal-oriented dialogs like setting an alarm or a explaining a catalogue\n","# Last, but not least, this way will enrich the vocabulary of our bot.\n","\n","# questions, answers = shuffle(np.array(questions), np.array(answers))\n","\n","print(questions[:3])\n","print(answers[:3])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Hey', 'i am awesome', 'and i own rental properties on the moon']\n","['Hello how may I help you?', 'of course you are', 'i doubt you own a property in the moon']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w5lJmjLEIys0","colab_type":"code","outputId":"c6576176-8715-424b-fcdb-198f0440fd0f","executionInfo":{"status":"ok","timestamp":1569837390108,"user_tz":-180,"elapsed":24508,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["NUM_DIALOGS = 5000\n","questions = questions[:NUM_DIALOGS]\n","answers = answers[:NUM_DIALOGS]\n","\n","print(len(answers))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OQ_KjYGQGmJR","colab_type":"text"},"source":["## Tokenizing\n","Let's tokenize our data using TensorFlow's preprocessing modules.\n","Also, we will add a special *start* and *end* token.\n","\n","The *end* token is useful for the decoder to know when to stop predicting words.\n","\n","The *start* token is also important for the decoder, since the decoder will progress by taking the tokens that it emits as inputs. So, before it has emitted anything it needs a special token to start with. \n","\n","Later, we will also try to import the GloVe embeddings to make use of transfer learning, something that will speed up our computations and make our model smarter.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"l8_g29712aDF","colab_type":"code","colab":{}},"source":["# Input: questions or answers matrix\n","#\n","# Returns: Modified matrix, with special tokens appended \n","# in the start/end of each string\n","def add_extra_tokens(matrix):\n","\n","  new_matrix = []\n","  for sequence in matrix:\n","    sequence = \"<start>\" + \" \" + sequence + \" \" + \"<end>\"\n","    new_matrix.append(sequence)\n","\n","  return new_matrix\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQ4LyqGY3gUm","colab_type":"code","colab":{}},"source":["questions = add_extra_tokens(questions) # Optional\n","answers = add_extra_tokens(answers)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0n1sisDT3lAY","colab_type":"code","outputId":"0fdd856a-0264-481b-8cf9-4d23061347ff","executionInfo":{"status":"ok","timestamp":1569837390115,"user_tz":-180,"elapsed":24468,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(questions[5:15])\n","print(answers[5:15])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['<start> and i programmed you <end>', '<start> Hey <end>', '<start> I am the king of the world <end>', '<start> I can have any woman I want! <end>', '<start> Even you bot, if I were in to AIs <end>', \"<start> Really? you're awfully agreeable aren't you <end>\", '<start> Having an agreement bot seems like a useless thing to have. I need some spice in my life! <end>', '<start> Hi <end>', '<start> Do you that I am a great person? <end>', '<start> I am only 6 inches tall. <end>']\n","['<start> i am the programmer <end>', '<start> Hello how may I help you? <end>', '<start> I agree that you are the king of the world <end>', '<start> I agree that you can have any woman you desire. <end>', '<start> Agreed. <end>', '<start> I agree that I am awfully agreeable, yes. <end>', '<start> I really agree with that. I am rather useles. <end>', '<start> Hello how may I help you? <end>', '<start> Yes! <end>', \"<start> That's correct! <end>\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"inJ4BVRa2WtT","colab_type":"code","colab":{}},"source":["# References: \n","#\n","# - TensorFlow Text preprocessing (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence)\n","# - TensorFlow Sequence preprocessing (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)\n","# - TensorFlow Advanced Tutorials (https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention)\n","\n","\n","# Returns the max length of a vector or a tensor\n","# This will be used for padding\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","# Returns the padded text and the tokenizer, based on a text corpus\n","def tokenize(lang):\n","  \n","  lang_tokenizer = Tokenizer(filters = '', lower=True) # Lowercase\n","  lang_tokenizer.fit_on_texts(lang) # Fit\n","\n","  tensor = lang_tokenizer.texts_to_sequences(lang) \n","\n","  tensor = pad_sequences(tensor, padding='post') # Pad to a fixed length\n","\n","  return tensor, lang_tokenizer\n","\n","# Returns encoder and decoder padded texts & tokenizer,\n","# given the input and target text\n","def load_dataset(input_text, target_text):\n","\n","  input_tensor, inp_lang_tokenizer = tokenize(input_text)\n","  target_tensor, targ_lang_tokenizer = tokenize(target_text)\n","\n","  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5hS8ACb60fr","colab_type":"code","colab":{}},"source":["# Load texts as tensors & tokenizers for encoder/decoder\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(questions, answers)\n","\n","# Calculate max_length of the target tensors\n","max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n","\n","# These will be used in the NN Design part"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLrnhz0O8Kqf","colab_type":"code","outputId":"ddd58310-0dd8-4cef-ca72-7c3d6d3e2fec","executionInfo":{"status":"ok","timestamp":1569837390627,"user_tz":-180,"elapsed":24933,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Creating training and validation sets using an 90%-10% split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n","\n","# Show lengths\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4500 4500 500 500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sGH1OTaY9I9Q","colab_type":"code","colab":{}},"source":["# Index to word mapping for input and target text, just like the tutorial\n","def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print(\"%d ----> %s\" % (t, lang.index_word[t]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKnEdWiO9K5C","colab_type":"code","outputId":"6bc69729-b894-4207-be50-e6caa030d1a5","executionInfo":{"status":"ok","timestamp":1569837390629,"user_tz":-180,"elapsed":24902,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["print(\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print()\n","print(\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input Language; index to word mapping\n","1 ----> <start>\n","803 ----> funny.\n","32 ----> thanks\n","9 ----> for\n","4 ----> the\n","34 ----> help\n","2 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","599 ----> anytime.\n","511 ----> agreed\n","26 ----> and\n","284 ----> take\n","687 ----> care\n","2 ----> <end>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OLq-EMFRvwco","colab_type":"text"},"source":["## Neural Network Parameters\n","\n","As always, there are no ideal values for the parameters. \n","\n","When hyperparameter tuning, we test values by trial-and-error.\n","Performance depends on many things and there is not a 'rule-them-all' approach.\n","\n","As in the description, there is no accurate metric for evaluating a chatbot.\n","\n","Of course, we can get a sense of loss from the cost function and using Keras/TF's history callbacks for validation, but text data is a bit different.\n","Other criteria, like User Experience (UX) or contexts must be taken into account. Also, one famous metric for such tasks is BLEU, but it's not in our context.\n","\n","(For example, a bot with stemming and lemmatization could perform better metrics-wise, but its outputs would be really weird to the user and they would not look so 'humanish' after all)\n","\n","The following final parameters are showcased, after many training and inference iterations. Of course, this is subject to change and the RAM/GPU capabilities are contributing with a major factor to this."]},{"cell_type":"code","metadata":{"id":"Yjy3aJrPjj0-","colab_type":"code","outputId":"9cc691d6-11be-4ec1-97a0-e352a56b1a62","executionInfo":{"status":"ok","timestamp":1569837390631,"user_tz":-180,"elapsed":24881,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(input_tensor_train))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ORSmKS-s9ruN","colab_type":"code","colab":{}},"source":["# Batch size:\n","# Typically, a power of 2 would be good for memory access reasons.\n","# If we define a higher batch size, there is a really high chance\n","# of the notebook crashing due to OOM (out-of-memory)\n","# In order to avoid RAM crashes, let's make it equal to 8, \n","# which seemed OK after many experiments\n","BATCH_SIZE = 8 \n","\n","BUFFER_SIZE = len(input_tensor_train)  \n","\n","# Use training steps\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE # Integer\n","\n","# Neurons\n","units = 16 # Instead of 512\n","\n","# Vocabulary sizes - used in Embeddings layers\n","vocab_inp_size = len(inp_lang.word_index) + 1 # Input\n","vocab_tar_size = len(targ_lang.word_index) + 1 # Target\n","embedding_dim = 50 # Instead of 256\n","\n","# Create a TF dataset\n","# Shuffle method uses a buffer of buffer_size elements,\n","# and randomly selects the next element from that buffer \n","\n","dataset = tf.data.Dataset.from_tensor_slices((\n","                            input_tensor_train, \n","                            target_tensor_train)).shuffle(BUFFER_SIZE) \n","\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # TODO: Remove if the following works\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"de1J7oLi-lsY","colab_type":"code","outputId":"7d3febdc-4758-453b-9c7b-1086271ecca4","executionInfo":{"status":"ok","timestamp":1569837390634,"user_tz":-180,"elapsed":24846,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["example_input_batch, example_target_batch = next(iter(dataset)) \n","example_input_batch.shape, example_target_batch.shape "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([Dimension(8), Dimension(40)]),\n"," TensorShape([Dimension(8), Dimension(47)]))"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"ACS-hxbZ6h0I","colab_type":"text"},"source":["Let's visualize the padded tokenized sentences that will be fed to our encoder-decoder neural network architecture."]},{"cell_type":"code","metadata":{"id":"d3M5-ORA3jP8","colab_type":"code","outputId":"530f1bfe-52f6-44c5-9bb0-a1948371a0a3","executionInfo":{"status":"ok","timestamp":1569837390635,"user_tz":-180,"elapsed":24814,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["print(example_input_batch[:2])\n","print(example_target_batch[:2])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[  1 152   3  43 217   3 166  16   4  33   2   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0]\n"," [  1  69   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0]], shape=(2, 40), dtype=int32)\n","tf.Tensor(\n","[[  1   4  51  30  79  19   7  44   2   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0]\n"," [  1   3  41 177   7 273  25  28   4  13  57  17  13  45 239 203   2   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0]], shape=(2, 47), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qnIWdhYl6825","colab_type":"text"},"source":["## Neural Network Design\n","\n","Followingly, we will implement a Sequence-to-sequence (seq2seq) model, like the one proposed by [Sutskever et al. in 2014](https://arxiv.org/abs/1409.3215).\n","\n","Such models are composed of an encoder and a decoder.\n","\n","An encoder builds an embedding vector, a sequence of numbers that represents the sentence meaning.\n","\n","A decoder, then, processes the sentence vector to emit a translation or generate a sentence.\n","\n","Each of them use a type of an RNN like a vanilla RNN, GRU, or LSTM, since we deal with sequential data.\n","\n","----\n","\n","Here is an example of applying Encoder-Decoder to a QA problem.\n","\n","![alt text](https://image.slidesharecdn.com/sequencelearningandmodernrnns-source-170603220404/95/sequence-learning-and-modern-rnns-46-638.jpg?cb=1496527568)\n"]},{"cell_type":"markdown","metadata":{"id":"dHhSPkG48eD1","colab_type":"text"},"source":["### Encoder\n","\n","The encoder takes the source language as an input - in our case the *questions* matrix."]},{"cell_type":"code","metadata":{"id":"avOGf7ov-Xix","colab_type":"code","colab":{}},"source":["# Based on TensorFlow's tutorial: https://github.com/tensorflow/nmt/tree/master/nmt\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n","    super(Encoder, self).__init__()\n","\n","    # Batch size & no. of units\n","    self.batch_size = batch_size\n","    self.enc_units = enc_units\n","    \n","    # Embedding layers convert the padded sentences into appropriate vectors\n","    #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #TODO: change\n","    self.embedding = tf.keras.layers.Embedding(vocab_size,\n","                                               embedding_dim,\n","                                               mask_zero=True) # Original\n","\n","    # We could use an LSTM or a simple RNN here\n","    # Although, GRU is simpler because it only returns 1 state and not 2.\n","    # We need to return the state to pass them to the decoder\n","\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   #return_sequences=True,\n","                                   return_state = True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  # Forward pass \n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state = hidden)\n","    return output, state\n","\n","  # Initialize the hidden state to zeros\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_size, self.enc_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIYItoh--cJ1","colab_type":"code","outputId":"93543c29-50ec-40f7-f5b2-6e43b6f28bbf","executionInfo":{"status":"ok","timestamp":1569837391643,"user_tz":-180,"elapsed":25775,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Instantiate the encoder with the \n","# corresponding vocabulary input size, embedding dimensions, units and batch size\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE) \n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Encoder output shape: (batch size, sequence length, units) (8, 16)\n","Encoder Hidden state shape: (batch size, units) (8, 16)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g8qGvzNaB7i0","colab_type":"text"},"source":["### Attention mechanism"]},{"cell_type":"markdown","metadata":{"id":"ndulqUxvLKBt","colab_type":"text"},"source":["We also add an attention mechanism as proposed by [Bahdanau, Cho, Bengio](https://arxiv.org/abs/1409.0473) in 2014.\n","\n","As seen in the following picture, the attention mechanism computes weights for each word in the encoder. The higher the weight, the more meaningful it is when attempting to decode.\n","\n","Note that attention mechanisms are mostly useful when having to deal with long sentences, like in applications of *text summarization, speech recognition,* etc.\n","\n","In our case, it may not be so useful, but let's keep it to experiment with it.\n","\n","Here's how weights take place in a Machine Translation problem.\n","\n","![alt text](https://miro.medium.com/max/1200/0*VrRTrruwf2BtW4t5.)"]},{"cell_type":"code","metadata":{"id":"4m5SsqfS_HHE","colab_type":"code","colab":{}},"source":["# As in the repo explained above\n","class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # hidden shape == (batch_size, hidden size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","    # we are doing this to perform addition to calculate the score\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDtjEwiB_JNp","colab_type":"code","outputId":"62153e8f-5f2f-4ad1-ca7a-8d7cce5c9bfe","executionInfo":{"status":"ok","timestamp":1569837391653,"user_tz":-180,"elapsed":25732,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Attention result shape: (batch size, units) (8, 16)\n","Attention weights shape: (batch_size, sequence_length, 1) (8, 8, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qwNpAS_7S7bF","colab_type":"text"},"source":["### Decoder\n","The encoded representation is then used by the **decoder** network to generate an output sequence."]},{"cell_type":"code","metadata":{"id":"zxQvxm2k-5Kl","colab_type":"code","colab":{}},"source":["# As in the repo\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n","    super(Decoder, self).__init__()\n","    self.batch_size = batch_size\n","    self.dec_units = dec_units\n","    #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # TODO: Change\n","    self.embedding = tf.keras.layers.Embedding(vocab_size,\n","                                               embedding_dim,\n","                                               mask_zero = True) # Original\n","    \n","    # Obviously, we need to return the generated sequences \n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    \n","    # A Dense Layer with units equal to the vocabulary size must take place.\n","    # With that way, we will follow with an argmax and generate a word\n","    # with index2word\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyMHJ7KV-76-","colab_type":"code","outputId":"3993c352-e080-4c8f-d0e1-8652c121d8d6","executionInfo":{"status":"ok","timestamp":1569837391658,"user_tz":-180,"elapsed":25665,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden,\n","                                      sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Decoder output shape: (batch_size, vocab size) (8, 3491)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YcOlz7q5VVmb","colab_type":"text"},"source":["### Loss Function & Optimizer\n","Let's the define the loss function and the optimizer that will be used when backpropagating.\n"]},{"cell_type":"code","metadata":{"id":"dsDGHFIn_O1R","colab_type":"code","colab":{}},"source":["# Define optimizer and loss object\n","\n","# RMSprop is one state-of-the-art optimizer in the literature, like Adam, etc.\n","#\n","# The categorical cross entropy loss function\n","# is the standard for multiclass classification.\n","optimizer = tf.keras.optimizers.RMSprop() \n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n","                                                            reduction='none')\n","\n","# Define the cost/loss function\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  # Return average mean from the batch\n","  return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EBSTlQZDCUjh","colab_type":"text"},"source":["### Training & Checkpoint"]},{"cell_type":"markdown","metadata":{"id":"3Le5T8rPV1c6","colab_type":"text"},"source":["Let's save checkpoints while training."]},{"cell_type":"code","metadata":{"id":"AwP0mn9K_gHO","colab_type":"code","colab":{}},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oaMKTUvmAWx_","colab_type":"code","colab":{}},"source":[" # Reference: https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention#training\n","@tf.function\n","def train_step(inp, targ, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","    dec_hidden = enc_hidden\n","\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, targ.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(targ[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, variables)\n","\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1j5uJHxlVhr","colab_type":"text"},"source":["Time for training."]},{"cell_type":"code","metadata":{"id":"osDp3HL9BxHm","colab_type":"code","outputId":"b4ba0ad7-3817-48a8-ba09-d9ccc172f382","executionInfo":{"status":"ok","timestamp":1569839266788,"user_tz":-180,"elapsed":1900704,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":782}},"source":["EPOCHS = 5 # Change to higher; care of overfitting\n","\n","for epoch in range(EPOCHS):\n","\n","  start = time.time()\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  # Apply weights/bias correction after *steps_per_epoch* times\n","  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","\n","    # A memory leak must be taking place here\n","    batch_loss = train_step(inp, targ, enc_hidden) \n","    total_loss += batch_loss\n","    \n","    # Does work only with eager execution\n","    if batch % 100 == 0:\n","        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch,\n","                                                     batch_loss.numpy())) \n","\n","  # saving (checkpoint) the model every 2 epochs\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n","  print('Time taken for one epoch {} sec\\n'.format(time.time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 1.4971\n","Epoch 1 Batch 100 Loss 1.3308\n","Epoch 1 Batch 200 Loss 1.3033\n","Epoch 1 Batch 300 Loss 1.2638\n","Epoch 1 Batch 400 Loss 0.3558\n","Epoch 1 Batch 500 Loss 1.1568\n","Epoch 1 Loss 0.9669\n","Time taken for one epoch 611.4609444141388 sec\n","\n","Epoch 2 Batch 0 Loss 0.8932\n","Epoch 2 Batch 100 Loss 1.1612\n","Epoch 2 Batch 200 Loss 1.2574\n","Epoch 2 Batch 300 Loss 1.2185\n","Epoch 2 Batch 400 Loss 0.3464\n","Epoch 2 Batch 500 Loss 1.1229\n","Epoch 2 Loss 0.8612\n","Time taken for one epoch 316.60665464401245 sec\n","\n","Epoch 3 Batch 0 Loss 0.8322\n","Epoch 3 Batch 100 Loss 1.1040\n","Epoch 3 Batch 200 Loss 1.2000\n","Epoch 3 Batch 300 Loss 1.1746\n","Epoch 3 Batch 400 Loss 0.3069\n","Epoch 3 Batch 500 Loss 1.0879\n","Epoch 3 Loss 0.8081\n","Time taken for one epoch 317.1367449760437 sec\n","\n","Epoch 4 Batch 0 Loss 0.7681\n","Epoch 4 Batch 100 Loss 1.0556\n","Epoch 4 Batch 200 Loss 1.1470\n","Epoch 4 Batch 300 Loss 1.1360\n","Epoch 4 Batch 400 Loss 0.2885\n","Epoch 4 Batch 500 Loss 1.0539\n","Epoch 4 Loss 0.7669\n","Time taken for one epoch 314.5347857475281 sec\n","\n","Epoch 5 Batch 0 Loss 0.7291\n","Epoch 5 Batch 100 Loss 1.0119\n","Epoch 5 Batch 200 Loss 1.1054\n","Epoch 5 Batch 300 Loss 1.1120\n","Epoch 5 Batch 400 Loss 0.2788\n","Epoch 5 Batch 500 Loss 1.0323\n","Epoch 5 Loss 0.7387\n","Time taken for one epoch 314.90559339523315 sec\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DdBzf99q2AtN","colab_type":"text"},"source":["Training takes around 40 minutes for only 4000 dialogs on 10 epochs.\n"]},{"cell_type":"markdown","metadata":{"id":"UaRocKSKCYy3","colab_type":"text"},"source":["## Inference\n","Now we need to take some input test sentences, preprocess them accordingly, pass them through the NN and take the 'index2word' values so we print out the chatbot's predicted sentence to the user."]},{"cell_type":"code","metadata":{"id":"tyXUkKBuLuKX","colab_type":"code","colab":{}},"source":["def evaluate(sentence):\n","\n","  # Preprocess the test sentence, just like we did \n","  sentence = preprocess_sentence(sentence)\n","\n","  # Replace the sentence with the word indices inside it\n","  sentence_array = sentence.split(' ')\n","  inputs = []\n","\n","  for i in sentence_array:\n","    if i in inp_lang.word_index: # Won't throw error if key isn't inside the vocab\n","      inputs.append(inp_lang.word_index[i])\n","\n","  # Pad it, just like before\n","  inputs = pad_sequences([inputs], maxlen = max_length_inp, padding='post')\n","\n","  # Convert to tensor\n","  inputs = tf.convert_to_tensor(inputs)\n","\n","  # Init \n","  result = '' # Empty string\n","\n","  hidden = [tf.zeros((1, units))] # Init hidden states to be passed to the encoder\n","  enc_out, enc_hidden = encoder(inputs, hidden) # Encoder output & hidden states\n","\n","  dec_hidden = enc_hidden # Hidden states to the encoder are passed to the decoder\n","\n","  # Decoder should know how to start\n","  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","  # Loop for maximum length of target\n","  for t in range(max_length_targ):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n","\n","    # Call argmax for the last Dense layer\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","    result += targ_lang.index_word[predicted_id] + ' '\n","\n","    # Stop when you predict the *end* token\n","    if targ_lang.index_word[predicted_id] == '<end>':\n","      result = result.replace('<end>', '') # Trim the end token for better UX\n","      return result, sentence#, attention_plot\n","\n","    # The predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, sentence#, attention_plot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lBVZ6pQNK7T","colab_type":"code","colab":{}},"source":["import re\n","\n","def preprocess_sentence(w):\n","    w = w.lower() # Conver to lowercase\n","\n","    # creating a space between a word and the punctuation following it\n","    # eg: \"he is a boy.\" => \"he is a boy .\"\n","    # Reference: https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","\n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","    w = w.rstrip().strip()\n","\n","    # adding a start and an end token to the sentence\n","    # so that the model knows when to start and stop predicting.\n","    w = '<start> ' + w + ' <end>'\n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qNczaBwMm3I","colab_type":"code","colab":{}},"source":["def generate(sentence):\n","    result, sentence = evaluate(sentence)\n","\n","    print('Input: %s' % (sentence))\n","    print('Predicted Bot Output: {}'.format(result))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnWl9H91Mykr","colab_type":"code","colab":{}},"source":["# Restore the latest checkpoint in checkpoint_dir\n","#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJadf6vYW6Pk","colab_type":"code","outputId":"6e68a472-4db0-4aad-812b-fda841a51ec0","executionInfo":{"status":"ok","timestamp":1569839268119,"user_tz":-180,"elapsed":1901898,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["text = \"hello\"\n","#text = \"set an alarm for me\"\n","generate(text)\n","\n","text = \"yes, you might help me. set an alarm for me\"\n","generate(text)\n","\n","text = \"whatever..what time is it now?\"\n","generate(text)\n","\n","text = \"bye\"\n","generate(text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input: <start> hello <end>\n","Predicted Bot Output: i help you?  \n","Input: <start> yes , you might help me . set an alarm for me <end>\n","Predicted Bot Output: i agree  \n","Input: <start> whatever . . what time is it now ? <end>\n","Predicted Bot Output: i help you?  \n","Input: <start> bye <end>\n","Predicted Bot Output: i agree  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bpHukw505gYW","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"ztqbx_7eTj2p","colab_type":"code","outputId":"aecac11c-070a-4660-93fb-7f11796496e9","executionInfo":{"status":"ok","timestamp":1569839268121,"user_tz":-180,"elapsed":1901875,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","\n","print(\"Enter /q to quit\")\n","while (1):\n","  \n","  user_input = input(\"User: \")\n","\n","  user_input = str(user_input)\n","\n","  if user_input == '/q':\n","    print(\"Quitting chat..\")\n","    break;\n","  else:\n","    print(\"Bot \" + str(generate(user_input)))\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nprint(\"Enter /q to quit\")\\nwhile (1):\\n  \\n  user_input = input(\"User: \")\\n\\n  user_input = str(user_input)\\n\\n  if user_input == \\'/q\\':\\n    print(\"Quitting chat..\")\\n    break;\\n  else:\\n    print(\"Bot \" + str(generate(user_input)))\\n'"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"6XCJgn4g-X9R","colab_type":"text"},"source":["-----------------"]},{"cell_type":"code","metadata":{"id":"MDvC_vMfgruE","colab_type":"code","outputId":"8314597d-e3af-421d-c2cb-d27ed025434e","executionInfo":{"status":"ok","timestamp":1569839268123,"user_tz":-180,"elapsed":1901848,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","This class allows to vectorize a text corpus, \n","by turning each text into either a sequence of integers\n","(each integer being the index of a token in a dictionary) or \n","into a vector where the coefficient for each token could be binary, \n","based on word count, based on tf-idf...\n","\n","\n","token = Tokenizer() # Lower-case letters\n","token.fit_on_texts(full_text) # Fit on the full text, not only questions or answers\n","# seq = token.texts_to_sequences(full_text) # Transform each text in texts to a sequence of integers\n","\n","word_index = token.word_index\n","print(len(word_index)) # Must be same as dictionary # TODO\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nThis class allows to vectorize a text corpus, \\nby turning each text into either a sequence of integers\\n(each integer being the index of a token in a dictionary) or \\ninto a vector where the coefficient for each token could be binary, \\nbased on word count, based on tf-idf...\\n\\n\\ntoken = Tokenizer() # Lower-case letters\\ntoken.fit_on_texts(full_text) # Fit on the full text, not only questions or answers\\n# seq = token.texts_to_sequences(full_text) # Transform each text in texts to a sequence of integers\\n\\nword_index = token.word_index\\nprint(len(word_index)) # Must be same as dictionary # TODO\\n'"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"mFyouCKUfwUk","colab_type":"code","outputId":"d4e097ae-34dc-48e1-9876-2a90f0efbb2e","executionInfo":{"status":"ok","timestamp":1569839268124,"user_tz":-180,"elapsed":1901830,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","# We add 1 for the unknown words\n","vocab_size = len(token.word_index) + 1 \n","# num_words = vocab_size #TODO\n","print(vocab_size) # Must be 35418 = 35417 + 1\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# We add 1 for the unknown words\\nvocab_size = len(token.word_index) + 1 \\n# num_words = vocab_size #TODO\\nprint(vocab_size) # Must be 35418 = 35417 + 1\\n'"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"Rz9kDfFrMeTC","colab_type":"text"},"source":["Let us import the GloVe embeddings.\n","\n","Note: This should take some time!\n","\n","Reference:  [Blog post from www.mc.ai](https://mc.ai/glove-word-embeddings-with-keras-python-code/)"]},{"cell_type":"code","metadata":{"id":"lMbwM5oOqM-f","colab_type":"code","outputId":"9a909951-0413-4262-b154-54878721808f","executionInfo":{"status":"ok","timestamp":1569839268124,"user_tz":-180,"elapsed":1901801,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","embeddings_index = {}\n","with open('glove.6B.50d.txt', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","\n","print(\"Glove Loded!\")\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nembeddings_index = {}\\nwith open(\\'glove.6B.50d.txt\\', encoding=\\'utf-8\\') as f:\\n    for line in f:\\n        values = line.split()\\n        word = values[0]\\n        coefs = np.asarray(values[1:], dtype=\\'float32\\')\\n        embeddings_index[word] = coefs\\n    f.close()\\n\\nprint(\"Glove Loded!\")\\n'"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"TFawaBfDHE7J","colab_type":"code","outputId":"75120846-cb14-4aca-c527-a4a69fa54246","executionInfo":{"status":"ok","timestamp":1569839268125,"user_tz":-180,"elapsed":1901732,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","embedding_vector = {} # Init\n","\n","# Local path to uploaded GloVe embeddings\n","# glove_path_300d1 = \"/content/drive/My Drive/Colab Notebooks/ncsr/data/embeddings/glove.6B.300d.txt\"\n","\n","glove_path = \"/content/drive/My Drive/Colab Notebooks/ncsr/data/embeddings/glove.6B.50d.txt\"\n","# f = open('./data/embeddings/glove.840B.300d.txt')\n","f = open(glove_path)\n","\n","# We create a dictionary called embedding_vector\n","# Each word in the vocabulary is a key and \n","# each key has a value that represents the word embeddings\n","for line in tqdm(f):\n","  value = line.split(' ')\n","  word = value[0]\n","  coef = np.array(value[1:], dtype = 'float32')\n","  embedding_vector[word] = coef\n","\n","# Close the pipe for the file, safety first\n","f.close()\n","\n","print('Found %s word vectors.' % len(embedding_vector))\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nembedding_vector = {} # Init\\n\\n# Local path to uploaded GloVe embeddings\\n# glove_path_300d1 = \"/content/drive/My Drive/Colab Notebooks/ncsr/data/embeddings/glove.6B.300d.txt\"\\n\\nglove_path = \"/content/drive/My Drive/Colab Notebooks/ncsr/data/embeddings/glove.6B.50d.txt\"\\n# f = open(\\'./data/embeddings/glove.840B.300d.txt\\')\\nf = open(glove_path)\\n\\n# We create a dictionary called embedding_vector\\n# Each word in the vocabulary is a key and \\n# each key has a value that represents the word embeddings\\nfor line in tqdm(f):\\n  value = line.split(\\' \\')\\n  word = value[0]\\n  coef = np.array(value[1:], dtype = \\'float32\\')\\n  embedding_vector[word] = coef\\n\\n# Close the pipe for the file, safety first\\nf.close()\\n\\nprint(\\'Found %s word vectors.\\' % len(embedding_vector))\\n'"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"NTMxZA6xSsiP","colab_type":"text"},"source":["Let's create a matrix which contains only the words present in our vocabulary and their corresponding embedding vector."]},{"cell_type":"code","metadata":{"id":"eAfswn8GRwb_","colab_type":"code","outputId":"86058ba0-fdb5-47bc-e40b-6a427952f7d6","executionInfo":{"status":"ok","timestamp":1569839268126,"user_tz":-180,"elapsed":1901716,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","embedding_matrix = np.zeros((vocab_size, embeddings_dim)) # Change dimension accordingly, as said above\n","\n","for word, i in tqdm(token.word_index.items()):\n","  embedding_value = embedding_vector.get(word)\n","  if embedding_value is not None:\n","    embedding_matrix[i] = embedding_value\n","    '''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nembedding_matrix = np.zeros((vocab_size, embeddings_dim)) # Change dimension accordingly, as said above\\n\\nfor word, i in tqdm(token.word_index.items()):\\n  embedding_value = embedding_vector.get(word)\\n  if embedding_value is not None:\\n    embedding_matrix[i] = embedding_value\\n    '"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"H_OT8ta-rSQh","colab_type":"code","outputId":"f70d2ebb-c015-42e9-d1e9-ff01dde1f679","executionInfo":{"status":"ok","timestamp":1569839268128,"user_tz":-180,"elapsed":1901681,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","embed_layer = Embedding(input_dim = VOCAB_SIZE, output_dim = embeddings_dim, trainable=True)\n","embed_layer.build((None,))\n","embed_layer.set_weights([embedding_matrix])\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nembed_layer = Embedding(input_dim = VOCAB_SIZE, output_dim = embeddings_dim, trainable=True)\\nembed_layer.build((None,))\\nembed_layer.set_weights([embedding_matrix])\\n'"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"RAGxzRw0bopk","colab_type":"text"},"source":["End\n","\n","-----------"]},{"cell_type":"markdown","metadata":{"id":"g2g1MKflWXlc","colab_type":"text"},"source":["## GloVe Embeddings\n","Let's try to import our GloVe embeddings so we have more meaningful representations of our words."]},{"cell_type":"code","metadata":{"id":"1BRvQGpiWdHB","colab_type":"code","outputId":"f2c0eb9f-dc0a-4504-f6dd-11bcc9e08109","executionInfo":{"status":"ok","timestamp":1569839268128,"user_tz":-180,"elapsed":1901664,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","import numpy as np # TODO: Transfer all imports in the first level\n"," \n","GLOVE_PATH =  \"/content/drive/My Drive/Colab Notebooks/ncsr/data/embeddings/glove.6B.50d.txt\"\n","GLOBE_VECTOR_LENGTH = 50 # Change accordingly - depends on which GloVe version you use\n","\n","# Read from the GloVe file\n","embedding_vector = {}\n","f = open(GLOVE_PATH)\n","for line in tqdm(f):\n","  value = line.split(' ')\n","  word = value[0]\n","  coef = np.array(value[1:],dtype = 'float32')\n","  embedding_vector[word] = coef\n","\n","\n","# Init embedding matrices\n","embedding_matrix_encoder = np.zeros((vocab_inp_size, embedding_dim))\n","embedding_matrix_decoder = np.zeros((vocab_tar_size, embedding_dim))\n","\n","# Populate\n","for word,i in tqdm(inp_lang.word_index.items()):\n","  embedding_value = embedding_vector.get(word)\n","  if embedding_value is not None:\n","    embedding_matrix_encoder[i] = embedding_value\n","\n","for word,i in tqdm(targ_lang.word_index.items()):\n","  embedding_value = embedding_vector.get(word)\n","  if embedding_value is not None:\n","    embedding_matrix_decoder[i] = embedding_value\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport numpy as np # TODO: Transfer all imports in the first level\\n \\nGLOVE_PATH =  \"/content/drive/My Drive/Colab Notebooks/ncsr/data/embeddings/glove.6B.50d.txt\"\\nGLOBE_VECTOR_LENGTH = 50 # Change accordingly - depends on which GloVe version you use\\n\\n# Read from the GloVe file\\nembedding_vector = {}\\nf = open(GLOVE_PATH)\\nfor line in tqdm(f):\\n  value = line.split(\\' \\')\\n  word = value[0]\\n  coef = np.array(value[1:],dtype = \\'float32\\')\\n  embedding_vector[word] = coef\\n\\n\\n# Init embedding matrices\\nembedding_matrix_encoder = np.zeros((vocab_inp_size, embedding_dim))\\nembedding_matrix_decoder = np.zeros((vocab_tar_size, embedding_dim))\\n\\n# Populate\\nfor word,i in tqdm(inp_lang.word_index.items()):\\n  embedding_value = embedding_vector.get(word)\\n  if embedding_value is not None:\\n    embedding_matrix_encoder[i] = embedding_value\\n\\nfor word,i in tqdm(targ_lang.word_index.items()):\\n  embedding_value = embedding_vector.get(word)\\n  if embedding_value is not None:\\n    embedding_matrix_decoder[i] = embedding_value\\n'"]},"metadata":{"tags":[]},"execution_count":47}]}]}