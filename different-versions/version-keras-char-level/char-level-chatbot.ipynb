{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"char-level-chatbot.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rsB-FSKncKH8","colab_type":"code","outputId":"8c4bc9df-0218-4960-8c8b-f755285f93b9","executionInfo":{"status":"ok","timestamp":1569679084350,"user_tz":-180,"elapsed":1392,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tvZavI2UcNyv","colab_type":"code","outputId":"6bb60a3f-d570-47d1-b242-4daff25a9f66","executionInfo":{"status":"ok","timestamp":1569679099004,"user_tz":-180,"elapsed":15983,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import os\n","!pwd\n","path_to_mount = '/content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level'\n","os.chdir(path_to_mount)\n","!ls\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","char-level-chatbot.ipynb  models\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"St8xplQEcfL6","colab_type":"code","outputId":"856394b4-7bf9-4e13-dcd3-9f0c0a7eb427","executionInfo":{"status":"ok","timestamp":1569679104099,"user_tz":-180,"elapsed":21043,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Import libraries\n","\n","# Parsing \n","import glob\n","import json\n","import random \n","import numpy as np\n","import pandas as pd \n","\n","# Preprocessing & NNs\n","from keras.models import Sequential, Model, load_model\n","from keras.layers import LSTM,Dense, Dropout, Embedding, CuDNNLSTM, Bidirectional, Embedding, Input, TimeDistributed\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","import re\n","import tensorflow as tf\n","#tf.enable_eager_execution() # evaluates operations immediately without building graphs\n","# Above does not work with placeholders\n","\n","# Etc\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import seaborn as sns\n","import os\n","import time\n","\n","%matplotlib inline"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0bNLRPRMchI1","colab_type":"code","outputId":"fc79bc45-0099-4093-e402-8fd00461814d","executionInfo":{"status":"ok","timestamp":1569679104101,"user_tz":-180,"elapsed":20924,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# Get absolute paths of files\n","path_to_mount = '/content/drive/My Drive/Colab Notebooks/ncsr/'\n","dialogues_regex_folder_path = \"/data/dialogues/*.txt\"\n","\n","# Get the absolute paths for each file \n","list_of_files = glob.glob(path_to_mount + dialogues_regex_folder_path)\n","print(list_of_files[:3]) # Visualize the first 3\n","print(len(list_of_files)) # 47; crashing? Try lower numbers\n","\n","#list_of_files = random.choices(population=list_of_files, k=10)\n","#print(len(list_of_files))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['/content/drive/My Drive/Colab Notebooks/ncsr//data/dialogues/AGREEMENT_BOT.txt', '/content/drive/My Drive/Colab Notebooks/ncsr//data/dialogues/APARTMENT_FINDER.txt', '/content/drive/My Drive/Colab Notebooks/ncsr//data/dialogues/CHECK_STATUS.txt']\n","47\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aZrG3jO4c8XU","colab_type":"code","outputId":"6247528f-31f7-4ffd-e8bf-cca5f04d5193","executionInfo":{"status":"ok","timestamp":1569679104546,"user_tz":-180,"elapsed":21340,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Parsing\n","list_of_dicts = [] # Init\n","\n","# Loop for each file\n","for filename in list_of_files:\n","  with open(filename) as f:\n","      for line in f: # Loop for each line (inside each file)\n","          list_of_dicts.append(json.loads(line)) # insert in a dictionary\n","\n","# Create a new dict containing only useful data\n","new_list_of_dicts = [] \n","\n","for old_dict in list_of_dicts:\n","  # foodict = {k: v for k, v in old_dict.items() if k.startswith('turns')} #TODO REMOVE\n","  foodict = {k: v for k, v in old_dict.items() if (k == 'turns')} \n","  new_list_of_dicts.append(foodict)\n","\n","list_of_dicts = [] # Free memory\n","list_of_dicts = new_list_of_dicts \n","\n","print(list_of_dicts[:2])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[{'turns': ['Hello how may I help you?', 'i am awesome', 'of course you are', 'and i own rental properties on the moon', 'i doubt you own a property in the moon', 'just kidding. i own them on Earth', \"that's a nice joke\", 'because i am a billionaire!', \"i don't seem to know you\", 'and i programmed you', 'i am the programmer']}, {'turns': ['Hello how may I help you?', 'I am the king of the world', 'I agree that you are the king of the world', 'I can have any woman I want!', 'I agree that you can have any woman you desire.', 'Even you bot, if I were in to AIs', 'Agreed.', \"Really? you're awfully agreeable aren't you\", 'I agree that I am awfully agreeable, yes.', 'Having an agreement bot seems like a useless thing to have. I need some spice in my life!', 'I really agree with that. I am rather useles.']}]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DdmkaPo5cndV","colab_type":"code","colab":{}},"source":["# Init matrices\n","questions = []\n","answers = []\n","\n","# We assume that the first answer by the bot (aka \"Hello, how may I help you?\") is returned after a user greeting\n","# This is used in order to ensure that the dataset will be even and each question is paired with an answeer.\n","# That's why we create a mini random catalog of artificial 'ghost' user greetings.\n","matrix_greetings = [\"Hey\", \"Hi\", \" \"]\n","\n","# A similar situation happens in the corner case when the last sentence is from the user.\n","# As said, each sentence from the user should be paired with a sentence from the bot.\n","# That's why we will in this case add an artificial one.\n","matrix_byes = [\"Ok\", \" \", \"Bye\"]\n","\n","# For each dictionary in the list\n","for dictionary in list_of_dicts:\n","  matrix_QA = dictionary['turns']\n","  \n","  # Append a first random greeting, as explained above\n","  #questions.append(random.choice(matrix_greetings))\n","    \n","  # In order to split the QAs to 2 matrices (questions & answers),\n","  # we will use a flag to indicate if the sentence is given from the bot or from the user\n","  #bot_flag = True # Init\n","\n","  # For each Q/A in the matrix\n","  matrix_QA.pop(0) # Remove \"hey how can i help you\"\n","\n","  bot_flag = False\n","  \n","  for sentence in matrix_QA:\n","\n","\n","    if bot_flag == True:\n","      answers.append(sentence) # Used for bot's answers\n","      bot_flag = False # Switch\n","      continue\n","\n","    else:\n","      questions.append(sentence) # Used for user's questions\n","      bot_flag = True # Switch\n","      #continue\n","\n","  # The last loop (ideally) ends with a bot's answer, thus making bot_flag equal to False.\n","  # Although, with data visualization and exploring, we can see that this does not happen all the time.\n","\n","  # Corner case: If the last answers was from the user, \n","  # then we need to add one artificial 'ghost' response from the bot to make the dataset even.\n","  if bot_flag == True: \n","    answers.append(random.choice(matrix_byes))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6I81jlBdG2q","colab_type":"code","outputId":"5f5e66d0-f942-481d-e63a-17e4231f4add","executionInfo":{"status":"ok","timestamp":1569679108225,"user_tz":-180,"elapsed":24976,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["assert len(questions) == len(answers), \"ERROR: The length of the questions and answer matrices are different.\"\n","# If it does not return any warning/error, then everything is good.\n","\n","print(len(questions)) # We have 238051 QAs (if we load all 47 texts)\n","\n","# Due to really high memory usage on TensorFlow training,\n","# we need to keep a lower number of dialogs.\n","# Also, we will shuffle them to ensure that our bot isn't overfitting on\n","# limited goal-oriented dialogs like setting an alarm or a exlplaining a catalogue\n","# Last, but not least, this way will enrich the vocabulary of our bot.\n","\n","questions, answers = shuffle(np.array(questions), np.array(answers))\n","\n","print(questions[:3])\n","print(answers[:3])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["200167\n","['Thanks for the info.' 'Richard who?' 'yes please']\n","[\"You're welcome\" 'williams'\n"," 'Okay I have gone ahead and change the three alarms next week to \"Feed Cat.\" Can I assist you with anything else?']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I9BVb_1kqgUY","colab_type":"code","outputId":"bbccfcce-7b59-4229-d226-9d5af9d88d2f","executionInfo":{"status":"ok","timestamp":1569679108227,"user_tz":-180,"elapsed":24958,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["NUM_DIALOGS = 40000\n","questions = questions[:NUM_DIALOGS]\n","answers = answers[:NUM_DIALOGS]\n","\n","print(len(answers))\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["40000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0kW3GQ8tdJ49","colab_type":"code","colab":{}},"source":["# Input: questions or answers matrix\n","#\n","# Returns: Modified matrix, with special tokens appended \n","# in the start/end of each string\n","\n","SOS_TOKEN = '\\t' # Start of Sentence\n","EOS_TOKEN = '\\n' # End of Sentence\n","\n","def add_extra_tokens(matrix):\n","\n","  new_matrix = []\n","  for sequence in matrix:\n","    sequence = SOS_TOKEN + \" \" + sequence + \" \" + EOS_TOKEN\n","    new_matrix.append(sequence)\n","\n","  return new_matrix\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcK__1-4dUkO","colab_type":"code","colab":{}},"source":["# questions = add_extra_tokens(questions)\n","answers = add_extra_tokens(answers)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eK1fo0XOda0E","colab_type":"code","colab":{}},"source":["input_characters = set()\n","target_characters = set()\n","\n","\n","for i in range(len(answers)):\n","    for char_input in questions[i]:\n","        if char_input not in input_characters:\n","            input_characters.add(char_input)\n","    \n","    for char_target in answers[i]:\n","        if char_target not in target_characters:\n","            target_characters.add(char_target)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVtJJXiPewxS","colab_type":"code","colab":{}},"source":["input_texts = []\n","input_texts = questions\n","\n","target_texts = []\n","target_texts = answers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTLfANLfeyEf","colab_type":"code","outputId":"49a9c585-2b05-4b2a-f5d0-d3d443089bd7","executionInfo":{"status":"ok","timestamp":1569679108840,"user_tz":-180,"elapsed":25473,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(input_texts[152])\n","print(target_texts[152])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Yes, I'm absolutely sure. Erase it please.\n","\t Your appointment for next Tuesday at 2:00.has been erased \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3_F5aWG4fFir","colab_type":"code","outputId":"234148ed-cd27-44f5-c27a-91b704e48306","executionInfo":{"status":"ok","timestamp":1569679108842,"user_tz":-180,"elapsed":25455,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print('Number of samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Number of samples: 40000\n","Number of unique input tokens: 90\n","Number of unique output tokens: 92\n","Max sequence length for inputs: 277\n","Max sequence length for outputs: 622\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pqhptblWfOF4","colab_type":"code","outputId":"706986ee-99e4-4b4f-ecd8-b67457fd3a19","executionInfo":{"status":"ok","timestamp":1569679108844,"user_tz":-180,"elapsed":25436,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(input_characters)\n","print(target_characters)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~']\n","['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tq0rpe1ofUSz","colab_type":"code","colab":{}},"source":["# Let's try one-hot encodings\n","\n","# Pretrained embeddings are better! Check GloVe or word2vec\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jlcxF0vfi2O","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# Check Memory Usage here\n","encoder_input_data = np.zeros(\n","  (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","  dtype='float32')\n","decoder_input_data = np.zeros(\n","  (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","  dtype='float32')\n","decoder_target_data = np.zeros(\n","  (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","  dtype='float32')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vulfrf0BgLm7","colab_type":"text"},"source":["The encoder_input_data will consist of samples (NUM_DIALOGS) of the maximum \n","sequence length (193) filled with the respective one-hot-encoded tokens (number of unique tokens) (in this case a vector of length 90)"]},{"cell_type":"code","metadata":{"id":"fx21_M-ifnZ5","colab_type":"code","outputId":"74649ba4-00fa-4a41-fde5-225ff3d54bb6","executionInfo":{"status":"ok","timestamp":1569679119107,"user_tz":-180,"elapsed":35641,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["encoder_input_data.shape"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(40000, 277, 90)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"1AfgtnVdgK3c","colab_type":"text"},"source":["\n","The decoder_input_data and the decoder_target_data are both constructed in the same way as the input data for the encoder. We need to construct those two sequences because we're training our model through a process called teacher forcing, where the decoder learns to generate decoder_target_data[t+1...] given decoder_input_data[...t] while taking into account the input sequence via the encoder's internal state. Therefore we have to offset decoder_target_data by one timestep.\n","\n","Time to fill in the data with the actual tokens. For that we iterate over all input and target texts and insert the respective one-hot encoding each character in the sequence."]},{"cell_type":"code","metadata":{"id":"L7Qy2022f2wP","colab_type":"code","colab":{}},"source":["for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","        \n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        \n","        if t > 0:\n","        # decoder_target_data will be ahead by one timestep\n","        # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYJGz1C5gm5O","colab_type":"code","outputId":"2191b8ea-a89c-4ef9-f7a9-2c3231b06618","executionInfo":{"status":"ok","timestamp":1569679121413,"user_tz":-180,"elapsed":37913,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["encoder_input_data[155].shape # (max inp length, no of unique tokens)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(277, 90)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"_BdkC3e0hSeG","colab_type":"text"},"source":["Now it's time to take a closer look at our encoder-decoder model. Our model will consist of two LSTMs. One will serve as an encoder, encoding the input sequence and producing internal state vectors which serve as conditioning for the decoder. The decoder, another LSTM, is responsible for predicting the individual characters of the target sequence. Its initial state is set to the state vectors from the encoder. This passes information about what to generate from the encoder to the decoder."]},{"cell_type":"code","metadata":{"id":"G3L7ZYsOhS5i","colab_type":"code","colab":{}},"source":["import keras, tensorflow\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDMW5GcRhYOn","colab_type":"code","colab":{}},"source":["latent_dim = 64  # latent dimensionality of the encoding space"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRa92yxOhcS6","colab_type":"code","outputId":"12bf2fee-602a-4b6a-b4a3-c96cc2a0e2bc","executionInfo":{"status":"ok","timestamp":1569679121416,"user_tz":-180,"elapsed":37851,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","\n","# Internal states should be fed to the decoder\n","encoder = LSTM(latent_dim, return_state=True) \n","\n","# We don't care about the encoder outputs\n","_, state_h, state_c = encoder(encoder_inputs)\n","\n","# Pass the hidden and cell state in a list\n","encoder_states = [state_h, state_c]"],"execution_count":23,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aixNOd1ih4DL","colab_type":"text"},"source":["When creating the LSTM, we now want it to return full output sequences as well as the internal state vectors. We're not using the decoder's internal states during training, but we will need them later for inference.\n","\n","\n","To arrive at the individual characters from the decoder's output we attach a Dense layer to the decoder's LSTM outputs where the number of units match the number of decoder tokens. This way we can just use a softmax activation for the dense layer's outputs and train the whole model using a categorical cross-entropy loss - a standard choice for classification problems."]},{"cell_type":"code","metadata":{"id":"E_EeUjOCh4bW","colab_type":"code","colab":{}},"source":["decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","\n","# We need to return both states (will be used during inference) and sequences\n","# Notice that the latent dimensionality here is the same for the encoder\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","\n","# Pass decoder inputs and encoder states for initialization\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states) \n","\n","# One unit for each decoder token and a softmax activation function\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","\n","decoder_outputs = decoder_dense(decoder_outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NhWme49jSBb","colab_type":"code","colab":{}},"source":["# Keras Functional API\n","\n","# Specify inputs and outputs\n","model = Model(inputs=[encoder_inputs, decoder_inputs], \n","              outputs=decoder_outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOEO0us9jaXx","colab_type":"code","outputId":"732f6343-f39f-489a-847d-15675189436c","executionInfo":{"status":"ok","timestamp":1569679121419,"user_tz":-180,"elapsed":37790,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["# Compile the Model\n","\n","# Define optimizer and loss function\n","# Categorical cross entropy is one standard loss function for multi-classification\n","\n","# Change optimizer & put objects\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.summary()"],"execution_count":26,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None, 90)     0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, None, 92)     0                                            \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 56), (None,  32928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, None, 56), ( 33376       input_2[0][0]                    \n","                                                                 lstm_1[0][1]                     \n","                                                                 lstm_1[0][2]                     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, None, 92)     5244        lstm_2[0][0]                     \n","==================================================================================================\n","Total params: 71,548\n","Trainable params: 71,548\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WGF6R4i1jpbY","colab_type":"code","outputId":"3d3734b6-5925-49c9-b9f1-fbae69ccdcd1","executionInfo":{"status":"ok","timestamp":1569680042516,"user_tz":-180,"elapsed":522089,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["VAL_SPLIT = 0.2 # Ratio of the data that we will validate on\n","BATCH_SIZE = 512  # Batch size for training\n","EPOCHS = 100 # Number of epochs to train for\n","\n","filepath= '/models/checkpoint.h5'\n","mounted ='/content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level'\n","filepath = mounted + filepath\n","\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n","                                save_best_only=True,\n","                                mode='min')\n","\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.02, patience=2,\n","                                verbose=0, \n","                                mode='auto')\n","\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size = BATCH_SIZE,\n","          epochs = EPOCHS,\n","          validation_split = VAL_SPLIT,\n","          callbacks=[checkpoint, early_stopping], \n","          verbose=1)\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Train on 32000 samples, validate on 8000 samples\n","Epoch 1/100\n","32000/32000 [==============================] - 104s 3ms/step - loss: 0.2005 - val_loss: 0.1982\n","\n","Epoch 00001: val_loss improved from inf to 0.19819, saving model to /content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level/models/checkpoint.h5\n","Epoch 2/100\n","32000/32000 [==============================] - 104s 3ms/step - loss: 0.1901 - val_loss: 0.1862\n","\n","Epoch 00002: val_loss improved from 0.19819 to 0.18617, saving model to /content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level/models/checkpoint.h5\n","Epoch 3/100\n","32000/32000 [==============================] - 104s 3ms/step - loss: 0.1783 - val_loss: 0.1759\n","\n","Epoch 00003: val_loss improved from 0.18617 to 0.17590, saving model to /content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level/models/checkpoint.h5\n","Epoch 4/100\n","32000/32000 [==============================] - 103s 3ms/step - loss: 0.1683 - val_loss: 0.1669\n","\n","Epoch 00004: val_loss improved from 0.17590 to 0.16687, saving model to /content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level/models/checkpoint.h5\n","Epoch 5/100\n","32000/32000 [==============================] - 104s 3ms/step - loss: 0.1606 - val_loss: 0.1601\n","\n","Epoch 00005: val_loss improved from 0.16687 to 0.16007, saving model to /content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level/models/checkpoint.h5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6251584a20>"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"yD-ir-Bzni4S","colab_type":"code","colab":{}},"source":["# Directory to save model\n","filepath= '/models/seq2seq-char.h5'\n","mounted ='/content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level'\n","filepath = mounted + filepath\n","\n","model.save(filepath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1R5ohckbkLvD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"045d56d3-585d-4d79-8105-bbef5a32f8c1","executionInfo":{"status":"ok","timestamp":1569679346343,"user_tz":-180,"elapsed":262659,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}}},"source":["'''\n","# Load model from directory\n","filepath= '/models/seq2seq-char.h5'\n","mounted ='/content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level'\n","filepath = mounted + filepath\n","\n","model = load_model(filepath)\n","'''"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# Load model from directory\\nfilepath= '/models/seq2seq-char.h5'\\nmounted ='/content/drive/My Drive/Colab Notebooks/ncsr/version-keras-char-level'\\nfilepath = mounted + filepath\\n\\nmodel = load_model(filepath)\\n\""]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"gu8ZQqfHk9tM","colab_type":"text"},"source":["\n","The inference mode works a bit differently than the training procedure. The procedure can be broken down into 4 steps:\n","\n","1. Encode the input sequence, return its internal states.\n","\n","2. Run the decoder using just the start-of-sequence character as input and the encoder internal states as the decoder's initial states.\n","\n","3. Append the character predicted (after lookup of the token) by the decoder to the decoded sequence.\n","\n","4. Repeat the process with the previously predicted character token as input and updates internal states."]},{"cell_type":"code","metadata":{"id":"c8LtCvt9k-Hk","colab_type":"code","colab":{}},"source":["# TODO: Serialize encoder inputs, states, latent dim\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","  decoder_inputs, initial_state=decoder_states_inputs)\n","\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","decoder_model = Model(\n","  [decoder_inputs] + decoder_states_inputs,\n","  [decoder_outputs] + decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"60cquRlOmir_","colab_type":"code","colab":{}},"source":["# reverse-lookup token index to turn sequences back to characters\n","reverse_input_char_index = dict(\n","  (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","  (i, char) for char, i in target_token_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWMm9mP7nwZX","colab_type":"text"},"source":["With that we can create a function to perform the whole process of decoding a given input sequence (inputs already tokenized)."]},{"cell_type":"code","metadata":{"id":"54FQ_IGUnw8-","colab_type":"code","colab":{}},"source":["def decode_sequence(input_seq):\n","  # encode the input sequence to get the internal state vectors.\n","  states_value = encoder_model.predict(input_seq)\n","  \n","  # generate empty target sequence of length 1 with only the start character\n","  target_seq = np.zeros((1, 1, num_decoder_tokens))\n","  target_seq[0, 0, target_token_index[SOS_TOKEN]] = 1.\n","  \n","  # output sequence loop\n","  stop_condition = False\n","  decoded_sentence = ''\n","  while not stop_condition:\n","    output_tokens, h, c = decoder_model.predict(\n","      [target_seq] + states_value)\n","    \n","    # sample a token and add the corresponding character to the \n","    # decoded sequence\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = reverse_target_char_index[sampled_token_index]\n","    decoded_sentence += sampled_char\n","    \n","    # check for the exit condition: either hitting max length\n","    # or predicting the 'stop' character\n","    if (sampled_char == EOS_TOKEN or \n","        len(decoded_sentence) > max_decoder_seq_length):\n","      stop_condition = True\n","      \n","    # update the target sequence (length 1).\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, sampled_token_index] = 1.\n","    \n","    # update states\n","    states_value = [h, c]\n","    \n","  return decoded_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DuSyu7-ao4ex","colab_type":"code","colab":{}},"source":["def answer_to(input_sentence):\n","    #input_sentence = \"How are you?\"\n","\n","    test_sentence_tokenized = np.zeros((1, max_encoder_seq_length,\n","                                num_encoder_tokens), dtype='float32')\n","\n","    for t, char in enumerate(input_sentence):\n","        test_sentence_tokenized[0, t, input_token_index[char]] = 1.\n","\n","    answer = decode_sequence(test_sentence_tokenized)\n","    \n","    return str(answer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pC2R6fYbOc7x","colab_type":"code","colab":{}},"source":["answer = answer_to(\"hello wassup\")\n","print(answer)\n","\n","answer = answer_to(\"do you have time for today?\")\n","print(answer)\n","\n","answer = answer_to(\"can i \")\n","print(answer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZX8gYFcWpuT5","colab_type":"code","outputId":"5ccfaf3b-a285-45d4-973f-d43f8ff717bc","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1569679397716,"user_tz":-180,"elapsed":313937,"user":{"displayName":"Eleftherios P. Loukas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsAhcH6SQ-uT3rQVERS-ipRfoEy3V34rMsB1lJcnI=s64","userId":"13830068770527141508"}}},"source":["print(\"Enter /q to quit\")\n","while (1):\n","  \n","  user_input = input(\"User: \")\n","\n","  user_input = str(user_input)\n","\n","  if user_input == '/q':\n","    print(\"Quitting chat..\")\n","    break;\n","  else:\n","    print(\"Bot \" + str(answer_to(user_input)))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Enter /q to quit\n","User: hi\n","Bot    a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n","User: hello\n","Bot    a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n","User: can you provide me with an example of the catalogue, pleaze/?\n","Bot    a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n","User: lol\n","Bot    a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n","User: /q\n","Quitting chat..\n"],"name":"stdout"}]}]}